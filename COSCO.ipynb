{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Required pip installs"
      ],
      "metadata": {
        "id": "GI9Ds-TD3R8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "90QrRNwpkGNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AuBbT8JmkORF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U aeon\n",
        "!pip install keras-self-attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkKrFN5HW_7a",
        "outputId": "e3e6ffa1-9c81-4f0e-e9f3-c8649a4469f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aeon\n",
            "  Downloading aeon-0.11.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.2.15)\n",
            "Requirement already satisfied: numba<0.61.0,>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon) (0.60.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (24.2)\n",
            "Collecting pandas<2.1.0,>=1.5.3 (from aeon)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scikit-learn<1.6.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.5.2)\n",
            "Collecting scipy<1.13.0,>=1.9.0 (from aeon)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (4.12.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon) (1.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba<0.61.0,>=0.55->aeon) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1.0,>=1.5.3->aeon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1.0,>=1.5.3->aeon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1.0,>=1.5.3->aeon) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6.0,>=1.0.0->aeon) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6.0,>=1.0.0->aeon) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.1.0,>=1.5.3->aeon) (1.16.0)\n",
            "Downloading aeon-0.11.1-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, pandas, aeon\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aeon-0.11.1 pandas-2.0.3 scipy-1.12.0\n",
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.26.4)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=0ae6454f0dd60b17ee5d3b5bd8fd83f6534ac959584a88111ba398d5a1a83b8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate Time Series Classification\n",
        "\n",
        "This notebook is a preview of the COSCO few-shot method. Please replace datasets with desired classification tasks. Current COSCO logic is used with ResNet, but can be applied to any Deep Learning model.\n",
        "\n",
        "\n",
        "## Parameters\n",
        "- LR: Learning Rate of model, default 0.01\n",
        "- rho: Neighborhood parameter for SAM, default 0.1\n",
        "- nEpoch: Number of epochs, default 100"
      ],
      "metadata": {
        "id": "xnbKPMYevqz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chnage parameters you would like to explore"
      ],
      "metadata": {
        "id": "PTrjE_Mg3JLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    #Hyperparameters\n",
        "    parser.add_argument('--lr', type=float, default=0.01)\n",
        "    parser.add_argument('--rho', type=float, default=0.1)\n",
        "    parser.add_argument('--nEpoch', type=int, default=100)\n",
        "\n",
        "    #Data Loading\n",
        "    parser.add_argument('--dataset', type=str, default='BasicMotions')\n",
        "    parser.add_argument('--shot', type=int, default=1 ,choices=[1,10])\n",
        "    parser.add_argument('--normalize', type=bool, default=False)\n",
        "\n",
        "    #Baseline Model\n",
        "    parser.add_argument('--model', type=str, default='resnet',choices=['resnet','tapnet'])\n",
        "\n",
        "    #SAM\n",
        "    parser.add_argument('--sam', type=bool, default=True)\n",
        "    parser.add_argument('--optimizer', type=str, default='adam',choices=['sgd','adam'])\n",
        "\n",
        "    #Prototypical Loss\n",
        "    parser.add_argument('--prototypical_loss', type=bool, default=True)\n",
        "\n",
        "\n",
        "    #Other Parameters\n",
        "    parser.add_argument('--prototypical_loss_type',type=str, default='neg',choices=['neg','sim','cos','negexp'])\n",
        "\n",
        "    #Saving\n",
        "    parser.add_argument('--save_dir', type=str, default='/content/classification_data/')\n",
        "    parser.add_argument('--save_name', type=str, default='results.csv')\n",
        "\n",
        "    return parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "args = get_args()\n",
        "print(args)"
      ],
      "metadata": {
        "id": "bPEQCyNHikA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b21650-3471-4582-85f5-17055a9770cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(lr=0.01, rho=0.1, nEpoch=100, dataset='BasicMotions', shot=1, normalize=False, model='resnet', sam=True, optimizer='adam', prototypical_loss=True, prototypical_loss_type='neg', save_dir='/content/classification_data/', save_name='results.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "-DmwlPF7tN_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JRB9/COSCO.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqJ_ie7_GIw8",
        "outputId": "c3dea206-e8c6-42f3-dc9b-f101a0959432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COSCO'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (291/291), done.\u001b[K\n",
            "remote: Compressing objects: 100% (241/241), done.\u001b[K\n",
            "remote: Total 445 (delta 75), reused 232 (delta 41), pack-reused 154 (from 1)\u001b[K\n",
            "Receiving objects: 100% (445/445), 82.32 MiB | 9.31 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "Updating files: 100% (225/225), done.\n",
            "Filtering content: 100% (80/80), 2.06 GiB | 58.85 MiB/s, done.\n",
            "Encountered 130 file(s) that should have been pointers, but weren't:\n",
            "\tDatasets/ArticularyWordRecognition/1-shot/X_train.npy\n",
            "\tDatasets/ArticularyWordRecognition/1-shot/original_indices.npy\n",
            "\tDatasets/ArticularyWordRecognition/1-shot/y_train.npy\n",
            "\tDatasets/ArticularyWordRecognition/10-shot/X_train.npy\n",
            "\tDatasets/ArticularyWordRecognition/10-shot/original_indices.npy\n",
            "\tDatasets/ArticularyWordRecognition/10-shot/y_train.npy\n",
            "\tDatasets/BasicMotions/1-shot/X_train.npy\n",
            "\tDatasets/BasicMotions/1-shot/original_indices.npy\n",
            "\tDatasets/BasicMotions/1-shot/y_train.npy\n",
            "\tDatasets/BasicMotions/10-shot/X_train.npy\n",
            "\tDatasets/BasicMotions/10-shot/original_indices.npy\n",
            "\tDatasets/BasicMotions/10-shot/y_train.npy\n",
            "\tDatasets/BasicMotions/X_test.npy\n",
            "\tDatasets/BasicMotions/X_train.npy\n",
            "\tDatasets/BasicMotions/y_test.npy\n",
            "\tDatasets/BasicMotions/y_train.npy\n",
            "\tDatasets/CharacterTrajectories/1-shot/X_train.npy\n",
            "\tDatasets/CharacterTrajectories/1-shot/original_indices.npy\n",
            "\tDatasets/CharacterTrajectories/1-shot/y_train.npy\n",
            "\tDatasets/CharacterTrajectories/10-shot/X_train.npy\n",
            "\tDatasets/CharacterTrajectories/10-shot/original_indices.npy\n",
            "\tDatasets/CharacterTrajectories/10-shot/y_train.npy\n",
            "\tDatasets/EigenWorms/1-shot/X_train.npy\n",
            "\tDatasets/EigenWorms/1-shot/original_indices.npy\n",
            "\tDatasets/EigenWorms/1-shot/y_train.npy\n",
            "\tDatasets/EigenWorms/10-shot/X_train.npy\n",
            "\tDatasets/EigenWorms/10-shot/original_indices.npy\n",
            "\tDatasets/EigenWorms/10-shot/y_train.npy\n",
            "\tDatasets/Epilepsy/1-shot/X_train.npy\n",
            "\tDatasets/Epilepsy/1-shot/original_indices.npy\n",
            "\tDatasets/Epilepsy/1-shot/y_train.npy\n",
            "\tDatasets/Epilepsy/10-shot/X_train.npy\n",
            "\tDatasets/Epilepsy/10-shot/original_indices.npy\n",
            "\tDatasets/Epilepsy/10-shot/y_train.npy\n",
            "\tDatasets/EthanolConcentration/1-shot/X_train.npy\n",
            "\tDatasets/EthanolConcentration/1-shot/original_indices.npy\n",
            "\tDatasets/EthanolConcentration/1-shot/y_train.npy\n",
            "\tDatasets/EthanolConcentration/10-shot/X_train.npy\n",
            "\tDatasets/EthanolConcentration/10-shot/original_indices.npy\n",
            "\tDatasets/EthanolConcentration/10-shot/y_train.npy\n",
            "\tDatasets/FaceDetection/1-shot/X_train.npy\n",
            "\tDatasets/FaceDetection/1-shot/original_indices.npy\n",
            "\tDatasets/FaceDetection/1-shot/y_train.npy\n",
            "\tDatasets/FaceDetection/10-shot/X_train.npy\n",
            "\tDatasets/FaceDetection/10-shot/original_indices.npy\n",
            "\tDatasets/FaceDetection/10-shot/y_train.npy\n",
            "\tDatasets/FingerMovements/1-shot/X_train.npy\n",
            "\tDatasets/FingerMovements/1-shot/original_indices.npy\n",
            "\tDatasets/FingerMovements/1-shot/y_train.npy\n",
            "\tDatasets/FingerMovements/10-shot/X_train.npy\n",
            "\tDatasets/FingerMovements/10-shot/original_indices.npy\n",
            "\tDatasets/FingerMovements/10-shot/y_train.npy\n",
            "\tDatasets/HandMovementDirection/1-shot/X_train.npy\n",
            "\tDatasets/HandMovementDirection/1-shot/original_indices.npy\n",
            "\tDatasets/HandMovementDirection/1-shot/y_train.npy\n",
            "\tDatasets/HandMovementDirection/10-shot/X_train.npy\n",
            "\tDatasets/HandMovementDirection/10-shot/original_indices.npy\n",
            "\tDatasets/HandMovementDirection/10-shot/y_train.npy\n",
            "\tDatasets/Heartbeat/1-shot/X_train.npy\n",
            "\tDatasets/Heartbeat/1-shot/original_indices.npy\n",
            "\tDatasets/Heartbeat/1-shot/y_train.npy\n",
            "\tDatasets/Heartbeat/10-shot/X_train.npy\n",
            "\tDatasets/Heartbeat/10-shot/original_indices.npy\n",
            "\tDatasets/Heartbeat/10-shot/y_train.npy\n",
            "\tDatasets/JapaneseVowels/1-shot/X_train.npy\n",
            "\tDatasets/JapaneseVowels/1-shot/original_indices.npy\n",
            "\tDatasets/JapaneseVowels/1-shot/y_train.npy\n",
            "\tDatasets/JapaneseVowels/10-shot/X_train.npy\n",
            "\tDatasets/JapaneseVowels/10-shot/original_indices.npy\n",
            "\tDatasets/JapaneseVowels/10-shot/y_train.npy\n",
            "\tDatasets/Libras/1-shot/X_train.npy\n",
            "\tDatasets/Libras/1-shot/original_indices.npy\n",
            "\tDatasets/Libras/1-shot/y_train.npy\n",
            "\tDatasets/Libras/10-shot/X_train.npy\n",
            "\tDatasets/Libras/10-shot/original_indices.npy\n",
            "\tDatasets/Libras/10-shot/y_train.npy\n",
            "\tDatasets/MotorImagery/1-shot/X_train.npy\n",
            "\tDatasets/MotorImagery/1-shot/original_indices.npy\n",
            "\tDatasets/MotorImagery/1-shot/y_train.npy\n",
            "\tDatasets/MotorImagery/10-shot/X_train.npy\n",
            "\tDatasets/MotorImagery/10-shot/original_indices.npy\n",
            "\tDatasets/MotorImagery/10-shot/y_train.npy\n",
            "\tDatasets/NATOPS/1-shot/X_train.npy\n",
            "\tDatasets/NATOPS/1-shot/original_indices.npy\n",
            "\tDatasets/NATOPS/1-shot/y_train.npy\n",
            "\tDatasets/NATOPS/10-shot/X_train.npy\n",
            "\tDatasets/NATOPS/10-shot/original_indices.npy\n",
            "\tDatasets/NATOPS/10-shot/y_train.npy\n",
            "\tDatasets/PEMS-SF/1-shot/X_train.npy\n",
            "\tDatasets/PEMS-SF/1-shot/original_indices.npy\n",
            "\tDatasets/PEMS-SF/1-shot/y_train.npy\n",
            "\tDatasets/PEMS-SF/10-shot/X_train.npy\n",
            "\tDatasets/PEMS-SF/10-shot/original_indices.npy\n",
            "\tDatasets/PEMS-SF/10-shot/y_train.npy\n",
            "\tDatasets/PenDigits/1-shot/X_train.npy\n",
            "\tDatasets/PenDigits/1-shot/original_indices.npy\n",
            "\tDatasets/PenDigits/1-shot/y_train.npy\n",
            "\tDatasets/PenDigits/10-shot/X_train.npy\n",
            "\tDatasets/PenDigits/10-shot/original_indices.npy\n",
            "\tDatasets/PenDigits/10-shot/y_train.npy\n",
            "\tDatasets/RacketSports/1-shot/X_train.npy\n",
            "\tDatasets/RacketSports/1-shot/original_indices.npy\n",
            "\tDatasets/RacketSports/1-shot/y_train.npy\n",
            "\tDatasets/RacketSports/10-shot/X_train.npy\n",
            "\tDatasets/RacketSports/10-shot/original_indices.npy\n",
            "\tDatasets/RacketSports/10-shot/y_train.npy\n",
            "\tDatasets/SelfRegulationSCP1/1-shot/X_train.npy\n",
            "\tDatasets/SelfRegulationSCP1/1-shot/original_indices.npy\n",
            "\tDatasets/SelfRegulationSCP1/1-shot/y_train.npy\n",
            "\tDatasets/SelfRegulationSCP1/10-shot/X_train.npy\n",
            "\tDatasets/SelfRegulationSCP1/10-shot/original_indices.npy\n",
            "\tDatasets/SelfRegulationSCP1/10-shot/y_train.npy\n",
            "\tDatasets/SelfRegulationSCP2/1-shot/X_train.npy\n",
            "\tDatasets/SelfRegulationSCP2/1-shot/original_indices.npy\n",
            "\tDatasets/SelfRegulationSCP2/1-shot/y_train.npy\n",
            "\tDatasets/SelfRegulationSCP2/10-shot/X_train.npy\n",
            "\tDatasets/SelfRegulationSCP2/10-shot/original_indices.npy\n",
            "\tDatasets/SelfRegulationSCP2/10-shot/y_train.npy\n",
            "\tDatasets/SpokenArabicDigits/1-shot/X_train.npy\n",
            "\tDatasets/SpokenArabicDigits/1-shot/original_indices.npy\n",
            "\tDatasets/SpokenArabicDigits/1-shot/y_train.npy\n",
            "\tDatasets/SpokenArabicDigits/10-shot/X_train.npy\n",
            "\tDatasets/SpokenArabicDigits/10-shot/original_indices.npy\n",
            "\tDatasets/SpokenArabicDigits/10-shot/y_train.npy\n",
            "\tDatasets/UWaveGestureLibrary/1-shot/X_train.npy\n",
            "\tDatasets/UWaveGestureLibrary/1-shot/original_indices.npy\n",
            "\tDatasets/UWaveGestureLibrary/1-shot/y_train.npy\n",
            "\tDatasets/UWaveGestureLibrary/10-shot/X_train.npy\n",
            "\tDatasets/UWaveGestureLibrary/10-shot/original_indices.npy\n",
            "\tDatasets/UWaveGestureLibrary/10-shot/y_train.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUdj0qPiqywQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import uuid\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prototypical Loss"
      ],
      "metadata": {
        "id": "bMqgpFi8zYo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.Prototypical_Loss import PrototypicalLoss"
      ],
      "metadata": {
        "id": "5HMiMlF4zYe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.Prototypical_Loss import prototypical_testing as ptest"
      ],
      "metadata": {
        "id": "HNJSc1pQMXaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet"
      ],
      "metadata": {
        "id": "RJKi_wD_EYfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.Baselines.ResNet import *"
      ],
      "metadata": {
        "id": "nfzYC5K1EYJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tapnet"
      ],
      "metadata": {
        "id": "wQ44iG5dWQs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.Baselines.TapNet import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s3QX9mYWR6L",
        "outputId": "f45d7d50-17f3-4cfd-a469-fe1ed83e662d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/aeon/base/__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAM Optimizer"
      ],
      "metadata": {
        "id": "wMJlFXunEdmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.SAM import SAM"
      ],
      "metadata": {
        "id": "aR_fP09FEgK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and Savind Data"
      ],
      "metadata": {
        "id": "Jom0rjzd39S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.utils.load_data import *\n",
        "from COSCO.utils.save import *"
      ],
      "metadata": {
        "id": "WK3EoWj_4F_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the training model"
      ],
      "metadata": {
        "id": "QVupUyWi4Hze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from COSCO.utils.proto_model import *"
      ],
      "metadata": {
        "id": "LMjnx8qm4OoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training"
      ],
      "metadata": {
        "id": "lGvvyySea-9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Instead of importing _BatchNorm, import the appropriate BatchNorm class\n",
        "from torch.nn import BatchNorm1d, BatchNorm2d, BatchNorm3d\n",
        "\n",
        "def disable_running_stats(model):\n",
        "    def _disable(module):\n",
        "        # Check if the module is an instance of any of the BatchNorm classes\n",
        "        if isinstance(module, (BatchNorm1d, BatchNorm2d, BatchNorm3d)):\n",
        "            module.backup_momentum = module.momentum\n",
        "            module.momentum = 0\n",
        "\n",
        "    model.apply(_disable)\n",
        "\n",
        "def enable_running_stats(model):\n",
        "    def _enable(module):\n",
        "        # Check if the module is an instance of any of the BatchNorm classes\n",
        "        if isinstance(module, (BatchNorm1d, BatchNorm2d, BatchNorm3d)) and hasattr(module, \"backup_momentum\"):\n",
        "            module.momentum = module.backup_momentum\n",
        "\n",
        "    model.apply(_enable)"
      ],
      "metadata": {
        "id": "n1MYKLL9vbXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same thing you have to make these all files\n",
        "\n",
        "def full_training(args):\n",
        "  train_data, train_label, test_data, test_label = load_data(args)\n",
        "\n",
        "  traindata = Dataset(train_data ,train_label)\n",
        "\n",
        "  input_size = train_data.shape[-1]\n",
        "\n",
        "  if args.model == \"tapnet\":\n",
        "     test_label =test_label.reshape(-1)\n",
        "     train_label = train_label.reshape(-1)\n",
        "\n",
        "  elif args.model==\"resnet\":\n",
        "    batch_size = 1024\n",
        "    trainloader = DataLoader(traindata, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=2)\n",
        "  acc = []\n",
        "  for i in range(5):\n",
        "    if args.model=='tapnet':\n",
        "        acc_tmp = train_tapnet(train_data, train_label, test_data, test_label, input_size,args)\n",
        "    elif args.model =='resnet':\n",
        "       acc_tmp = proto_neg_train_model(trainloader, train_label, test_data, test_label, input_size,args)\n",
        "    print(i)\n",
        "    acc.append(acc_tmp)\n",
        "\n",
        "  acc = np.array(acc)\n",
        "\n",
        "  # save the data\n",
        "  save_to_file_directory(acc,args)\n",
        "\n",
        "  # save to dataframe\n",
        "  save_to_dataframe(acc,args)\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "rtupXhHisWBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns for our results dataframe\n",
        "columns = [\"Dataset\", \"Shots\", \"Normalization\", \"Result\"]\n",
        "\n",
        "# dataframe construction\n",
        "df = pd.DataFrame(columns = columns)\n",
        "\n",
        "# filepath for our csv\n",
        "filepath = args.save_dir + args.save_name\n",
        "\n",
        "os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "# creating empty df and csv\n",
        "df.to_csv(filepath, index=False)\n",
        "\n",
        "for dataset_name in [args.dataset]:\n",
        "  for shot_dir in [args.shot]:\n",
        "    full_training(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6-pZ_1nB-Tw",
        "outputId": "b71d64d2-6d42-44ed-bc02-0fa2987f2ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/COSCO/utils/proto_model.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  outputs1 = model_resnet(torch.tensor(inputs).transpose(1,2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 --> 0.7946372032165527 0.7946372032165527 0.7926415801048279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/COSCO/utils/proto_model.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tmp = criterion(model_resnet(torch.tensor(inputs).transpose(1,2).float())[1], labels)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 --> 0.7940773963928223 0.7940773963928223 0.8071625232696533\n",
            "Epoch: 3 --> 0.786394476890564 0.786394476890564 0.7703090906143188\n",
            "Epoch: 4 --> 0.7779690027236938 0.7779690027236938 0.7599457502365112\n",
            "Epoch: 5 --> 0.7714455127716064 0.7714455127716064 0.7559036016464233\n",
            "Epoch: 6 --> 0.7668169736862183 0.7668169736862183 0.7566694617271423\n",
            "Epoch: 7 --> 0.7624586224555969 0.7624586224555969 0.760635495185852\n",
            "Epoch: 8 --> 0.7588100433349609 0.7588100433349609 0.7561649084091187\n",
            "Epoch: 9 --> 0.7561074495315552 0.7561074495315552 0.7547019124031067\n",
            "Epoch: 10 --> 0.7541137933731079 0.7541137933731079 0.7533724904060364\n",
            "Epoch: 11 --> 0.7526184320449829 0.7526184320449829 0.752106249332428\n",
            "Epoch: 12 --> 0.7514823079109192 0.7514823079109192 0.7512405514717102\n",
            "Epoch: 13 --> 0.7505436539649963 0.7505436539649963 0.7508639097213745\n",
            "Epoch: 14 --> 0.7497263550758362 0.7497263550758362 0.7520103454589844\n",
            "Epoch: 15 --> 0.7489176988601685 0.7489176988601685 0.751502275466919\n",
            "Epoch: 16 --> 0.7482767701148987 0.7482767701148987 0.7509016990661621\n",
            "Epoch: 17 --> 0.747744619846344 0.747744619846344 0.7480049133300781\n",
            "Epoch: 18 --> 0.7472031116485596 0.7472031116485596 0.7465307712554932\n",
            "Epoch: 19 --> 0.7467215061187744 0.7467215061187744 0.7460583448410034\n",
            "Epoch: 20 --> 0.7463150024414062 0.7463150024414062 0.7457600831985474\n",
            "Epoch: 21 --> 0.7459864020347595 0.7459864020347595 0.7454794049263\n",
            "Epoch: 22 --> 0.7457244992256165 0.7457244992256165 0.7452536225318909\n",
            "Epoch: 23 --> 0.7455135583877563 0.7455135583877563 0.745059072971344\n",
            "Epoch: 24 --> 0.7453441619873047 0.7453441619873047 0.7448854446411133\n",
            "Epoch: 25 --> 0.7452057003974915 0.7452057003974915 0.744749903678894\n",
            "Epoch: 26 --> 0.7450913190841675 0.7450913190841675 0.7446216344833374\n",
            "Epoch: 27 --> 0.7449967861175537 0.7449967861175537 0.7445142865180969\n",
            "Epoch: 28 --> 0.7449180483818054 0.7449180483818054 0.7444290518760681\n",
            "Epoch: 29 --> 0.744851291179657 0.744851291179657 0.7443503737449646\n",
            "Epoch: 30 --> 0.744794487953186 0.744794487953186 0.7442863583564758\n",
            "Epoch: 31 --> 0.744745671749115 0.744745671749115 0.7442318201065063\n",
            "Epoch: 32 --> 0.7447041869163513 0.7447041869163513 0.744184136390686\n",
            "Epoch: 33 --> 0.7446690201759338 0.7446690201759338 0.7441444396972656\n",
            "Epoch: 34 --> 0.7446390390396118 0.7446390390396118 0.7441107034683228\n",
            "Epoch: 35 --> 0.7446129322052002 0.7446129322052002 0.7440805435180664\n",
            "Epoch: 36 --> 0.744590163230896 0.744590163230896 0.7440546751022339\n",
            "Epoch: 37 --> 0.7445704340934753 0.7445704340934753 0.7440319061279297\n",
            "Epoch: 38 --> 0.7445531487464905 0.7445531487464905 0.7440109252929688\n",
            "Epoch: 39 --> 0.7445383071899414 0.7445383071899414 0.7439929246902466\n",
            "Epoch: 40 --> 0.7445254325866699 0.7445254325866699 0.7439766526222229\n",
            "Epoch: 41 --> 0.7445142865180969 0.7445142865180969 0.7439611554145813\n",
            "Epoch: 42 --> 0.7445046901702881 0.7445046901702881 0.7439483404159546\n",
            "Epoch: 43 --> 0.74449622631073 0.74449622631073 0.7439367771148682\n",
            "Epoch: 44 --> 0.7444888949394226 0.7444888949394226 0.7439268827438354\n",
            "Epoch: 45 --> 0.7444825172424316 0.7444825172424316 0.7439167499542236\n",
            "Epoch: 46 --> 0.7444770932197571 0.7444770932197571 0.7439083456993103\n",
            "Epoch: 47 --> 0.7444722652435303 0.7444722652435303 0.7438995838165283\n",
            "Epoch: 48 --> 0.7444682717323303 0.7444682717323303 0.7438923120498657\n",
            "Epoch: 49 --> 0.7444648742675781 0.7444648742675781 0.7438851594924927\n",
            "Epoch: 50 --> 0.7444619536399841 0.7444619536399841 0.7438788414001465\n",
            "Epoch: 51 --> 0.7444595098495483 0.7444595098495483 0.7438727617263794\n",
            "Epoch: 52 --> 0.7444575428962708 0.7444575428962708 0.7438676953315735\n",
            "Epoch: 53 --> 0.7444560527801514 0.7444560527801514 0.7438626289367676\n",
            "Epoch: 54 --> 0.7444547414779663 0.7444547414779663 0.7438578009605408\n",
            "Epoch: 55 --> 0.7444538474082947 0.7444538474082947 0.7438532114028931\n",
            "Epoch: 56 --> 0.7444530129432678 0.7444530129432678 0.7438491582870483\n",
            "Epoch: 57 --> 0.7444525361061096 0.7444525361061096 0.7438451051712036\n",
            "Epoch: 58 --> 0.7444522976875305 0.7444522976875305 0.7438416481018066\n",
            "Epoch: 59 --> 0.7444522380828857 0.7444522380828857 0.743838369846344\n",
            "Epoch: 60 --> 0.7444522380828857 0.7444522380828857 0.7438350915908813\n",
            "Epoch: 61 --> 0.7444524168968201 0.7444524168968201 0.7438320517539978\n",
            "Epoch: 62 --> 0.7444527745246887 0.7444527745246887 0.743829071521759\n",
            "Epoch: 63 --> 0.7444531917572021 0.7444531917572021 0.7438263893127441\n",
            "Epoch: 64 --> 0.7444537281990051 0.7444537281990051 0.7438238263130188\n",
            "Epoch: 65 --> 0.7444542646408081 0.7444542646408081 0.743821382522583\n",
            "Epoch: 66 --> 0.7444548606872559 0.7444548606872559 0.743818998336792\n",
            "Epoch: 67 --> 0.7444555759429932 0.7444555759429932 0.7438168525695801\n",
            "Epoch: 68 --> 0.74445641040802 0.74445641040802 0.7438147068023682\n",
            "Epoch: 69 --> 0.7444572448730469 0.7444572448730469 0.7438129186630249\n",
            "Epoch: 70 --> 0.7444580793380737 0.7444580793380737 0.7438108921051025\n",
            "Epoch: 71 --> 0.7444589138031006 0.7444589138031006 0.7438088655471802\n",
            "Epoch: 72 --> 0.744459867477417 0.744459867477417 0.7438070774078369\n",
            "Epoch: 73 --> 0.7444608211517334 0.7444608211517334 0.7438052892684937\n",
            "Epoch: 74 --> 0.7444618344306946 0.7444618344306946 0.7438035011291504\n",
            "Epoch: 75 --> 0.7444628477096558 0.7444628477096558 0.7438017129898071\n",
            "Epoch: 76 --> 0.7444638609886169 0.7444638609886169 0.743800163269043\n",
            "Epoch: 77 --> 0.7444648742675781 0.7444648742675781 0.7437986135482788\n",
            "Epoch: 78 --> 0.7444659471511841 0.7444659471511841 0.7437971830368042\n",
            "Epoch: 79 --> 0.7444669604301453 0.7444669604301453 0.7437958121299744\n",
            "Epoch: 80 --> 0.744468092918396 0.744468092918396 0.743794322013855\n",
            "Epoch: 81 --> 0.744469165802002 0.744469165802002 0.7437930107116699\n",
            "Epoch: 82 --> 0.7444701790809631 0.7444701790809631 0.7437916994094849\n",
            "Epoch: 83 --> 0.7444713115692139 0.7444713115692139 0.7437905073165894\n",
            "Epoch: 84 --> 0.744472324848175 0.744472324848175 0.7437892556190491\n",
            "Epoch: 85 --> 0.7444734573364258 0.7444734573364258 0.7437880039215088\n",
            "Epoch: 86 --> 0.7444745302200317 0.7444745302200317 0.7437868118286133\n",
            "Epoch: 87 --> 0.7444756031036377 0.7444756031036377 0.7437856793403625\n",
            "Epoch: 88 --> 0.7444766163825989 0.7444766163825989 0.7437846064567566\n",
            "Epoch: 89 --> 0.7444776296615601 0.7444776296615601 0.7437835931777954\n",
            "Epoch: 90 --> 0.744478702545166 0.744478702545166 0.7437824010848999\n",
            "Epoch: 91 --> 0.7444797158241272 0.7444797158241272 0.7437813878059387\n",
            "Epoch: 92 --> 0.7444808483123779 0.7444808483123779 0.7437804937362671\n",
            "Epoch: 93 --> 0.7444817423820496 0.7444817423820496 0.7437794804573059\n",
            "Epoch: 94 --> 0.7444828748703003 0.7444828748703003 0.7437787652015686\n",
            "Epoch: 95 --> 0.7444838285446167 0.7444838285446167 0.743777871131897\n",
            "Epoch: 96 --> 0.7444848418235779 0.7444848418235779 0.7437769174575806\n",
            "Epoch: 97 --> 0.7444858551025391 0.7444858551025391 0.7437760829925537\n",
            "Epoch: 98 --> 0.7444868087768555 0.7444868087768555 0.7437751889228821\n",
            "Epoch: 99 --> 0.7444878220558167 0.7444878220558167 0.7437744140625\n",
            "torch.Size([4, 128])\n",
            "Epoch: 100 --> 0.7444887757301331 0.7444887757301331 0.7437735795974731\n",
            "Finished Training\n",
            "Final Accuracy:  1.0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/COSCO/utils/proto_model.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_centroids = torch.load('train_centroids.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 --> 0.7814762592315674 0.7814762592315674 0.7740033864974976\n",
            "Epoch: 2 --> 0.7838499546051025 0.7838499546051025 0.769862174987793\n",
            "Epoch: 3 --> 0.784179151058197 0.784179151058197 0.7691371440887451\n",
            "Epoch: 4 --> 0.7743150591850281 0.7743150591850281 0.758507490158081\n",
            "Epoch: 5 --> 0.7624194622039795 0.7624194622039795 0.7542306780815125\n",
            "Epoch: 6 --> 0.7558296918869019 0.7558296918869019 0.7534761428833008\n",
            "Epoch: 7 --> 0.7526492476463318 0.7526492476463318 0.7510681748390198\n",
            "Epoch: 8 --> 0.7510296702384949 0.7510296702384949 0.7494230270385742\n",
            "Epoch: 9 --> 0.7501491904258728 0.7501491904258728 0.7482689619064331\n",
            "Epoch: 10 --> 0.7496393918991089 0.7496393918991089 0.7475621700286865\n",
            "Epoch: 11 --> 0.7492998838424683 0.7492998838424683 0.7470522522926331\n",
            "Epoch: 12 --> 0.7490816712379456 0.7490816712379456 0.7465749979019165\n",
            "Epoch: 13 --> 0.7489213943481445 0.7489213943481445 0.7461759448051453\n",
            "Epoch: 14 --> 0.7487688064575195 0.7487688064575195 0.7459506988525391\n",
            "Epoch: 15 --> 0.7486166954040527 0.7486166954040527 0.7459156513214111\n",
            "Epoch: 16 --> 0.7484372854232788 0.7484372854232788 0.7463530898094177\n",
            "Epoch: 17 --> 0.7481844425201416 0.7481844425201416 0.7512935400009155\n",
            "Epoch: 18 --> 0.7474538087844849 0.7474538087844849 0.7470436096191406\n",
            "Epoch: 19 --> 0.746269166469574 0.746269166469574 0.7461770176887512\n",
            "Epoch: 20 --> 0.7454442977905273 0.7454442977905273 0.7461977005004883\n",
            "Epoch: 21 --> 0.7449865341186523 0.7449865341186523 0.7464616894721985\n",
            "Epoch: 22 --> 0.7447157502174377 0.7447157502174377 0.7468222379684448\n",
            "Epoch: 23 --> 0.7445401549339294 0.7445401549339294 0.7472091913223267\n",
            "Epoch: 24 --> 0.7444156408309937 0.7444156408309937 0.7475162148475647\n",
            "Epoch: 25 --> 0.7443215250968933 0.7443215250968933 0.7476073503494263\n",
            "Epoch: 26 --> 0.7442474365234375 0.7442474365234375 0.7474424839019775\n",
            "Epoch: 27 --> 0.7441877126693726 0.7441877126693726 0.7471112012863159\n",
            "Epoch: 28 --> 0.744139552116394 0.744139552116394 0.7466433048248291\n",
            "Epoch: 29 --> 0.7441003918647766 0.7441003918647766 0.7461814284324646\n",
            "Epoch: 30 --> 0.7440686225891113 0.7440686225891113 0.745734691619873\n",
            "Epoch: 31 --> 0.7440428733825684 0.7440428733825684 0.7453130483627319\n",
            "Epoch: 32 --> 0.7440218329429626 0.7440218329429626 0.7450027465820312\n",
            "Epoch: 33 --> 0.7440046072006226 0.7440046072006226 0.7447447180747986\n",
            "Epoch: 34 --> 0.7439906597137451 0.7439906597137451 0.7445547580718994\n",
            "Epoch: 35 --> 0.7439795732498169 0.7439795732498169 0.74440598487854\n",
            "Epoch: 36 --> 0.7439708709716797 0.7439708709716797 0.7442960739135742\n",
            "Epoch: 37 --> 0.743963897228241 0.743963897228241 0.7442103028297424\n",
            "Epoch: 38 --> 0.7439586520195007 0.7439586520195007 0.7441446781158447\n",
            "Epoch: 39 --> 0.7439544796943665 0.7439544796943665 0.7440977096557617\n",
            "Epoch: 40 --> 0.7439510226249695 0.7439510226249695 0.7440590858459473\n",
            "Epoch: 41 --> 0.7439483404159546 0.7439483404159546 0.7440252900123596\n",
            "Epoch: 42 --> 0.7439462542533875 0.7439462542533875 0.7440016269683838\n",
            "Epoch: 43 --> 0.7439445853233337 0.7439445853233337 0.7439806461334229\n",
            "Epoch: 44 --> 0.7439431548118591 0.7439431548118591 0.7439623475074768\n",
            "Epoch: 45 --> 0.7439420223236084 0.7439420223236084 0.7439478635787964\n",
            "Epoch: 46 --> 0.743941068649292 0.743941068649292 0.7439345121383667\n",
            "Epoch: 47 --> 0.7439402341842651 0.7439402341842651 0.7439227104187012\n",
            "Epoch: 48 --> 0.7439396381378174 0.7439396381378174 0.743912935256958\n",
            "Epoch: 49 --> 0.7439390420913696 0.7439390420913696 0.7439037561416626\n",
            "Epoch: 50 --> 0.7439385652542114 0.7439385652542114 0.7438954710960388\n",
            "Epoch: 51 --> 0.7439380884170532 0.7439380884170532 0.7438884377479553\n",
            "Epoch: 52 --> 0.7439377307891846 0.7439377307891846 0.74388188123703\n",
            "Epoch: 53 --> 0.7439374923706055 0.7439374923706055 0.743876576423645\n",
            "Epoch: 54 --> 0.7439371347427368 0.7439371347427368 0.74387127161026\n",
            "Epoch: 55 --> 0.7439368963241577 0.7439368963241577 0.7438663244247437\n",
            "Epoch: 56 --> 0.7439366579055786 0.7439366579055786 0.7438614368438721\n",
            "Epoch: 57 --> 0.7439364790916443 0.7439364790916443 0.7438573241233826\n",
            "Epoch: 58 --> 0.74393630027771 0.74393630027771 0.7438534498214722\n",
            "Epoch: 59 --> 0.7439360618591309 0.7439360618591309 0.7438498735427856\n",
            "Epoch: 60 --> 0.7439359426498413 0.7439359426498413 0.7438466548919678\n",
            "Epoch: 61 --> 0.7439358234405518 0.7439358234405518 0.7438432574272156\n",
            "Epoch: 62 --> 0.7439355850219727 0.7439355850219727 0.743840217590332\n",
            "Epoch: 63 --> 0.7439355254173279 0.7439355254173279 0.7438375353813171\n",
            "Epoch: 64 --> 0.7439354658126831 0.7439354658126831 0.7438342571258545\n",
            "Epoch: 65 --> 0.7439353466033936 0.7439353466033936 0.7438318729400635\n",
            "Epoch: 66 --> 0.743935227394104 0.743935227394104 0.7438298463821411\n",
            "Epoch: 67 --> 0.7439349889755249 0.7439349889755249 0.7438284158706665\n",
            "Epoch: 68 --> 0.7439349889755249 0.7439349889755249 0.7438265085220337\n",
            "Epoch: 69 --> 0.7439347505569458 0.7439347505569458 0.7438255548477173\n",
            "Epoch: 70 --> 0.7439346313476562 0.7439346313476562 0.7438240051269531\n",
            "Epoch: 71 --> 0.7439345121383667 0.7439345121383667 0.7438220381736755\n",
            "Epoch: 72 --> 0.7439343929290771 0.7439343929290771 0.7438200116157532\n",
            "Epoch: 73 --> 0.7439342737197876 0.7439342737197876 0.7438187003135681\n",
            "Epoch: 74 --> 0.743934154510498 0.743934154510498 0.7438175082206726\n",
            "Epoch: 75 --> 0.7439340353012085 0.7439340353012085 0.7438158392906189\n",
            "Epoch: 76 --> 0.743933916091919 0.743933916091919 0.7438147068023682\n",
            "Epoch: 77 --> 0.7439338564872742 0.7439338564872742 0.7438132166862488\n",
            "Epoch: 78 --> 0.7439336776733398 0.7439336776733398 0.7438119649887085\n",
            "Epoch: 79 --> 0.7439336180686951 0.7439336180686951 0.7438107132911682\n",
            "Epoch: 80 --> 0.7439334988594055 0.7439334988594055 0.7438096404075623\n",
            "Epoch: 81 --> 0.7439333200454712 0.7439333200454712 0.7438086271286011\n",
            "Epoch: 82 --> 0.7439333200454712 0.7439333200454712 0.7438076138496399\n",
            "Epoch: 83 --> 0.7439332008361816 0.7439332008361816 0.7438066601753235\n",
            "Epoch: 84 --> 0.7439330816268921 0.7439330816268921 0.7438060641288757\n",
            "Epoch: 85 --> 0.7439329624176025 0.7439329624176025 0.7438054084777832\n",
            "Epoch: 86 --> 0.7439329624176025 0.7439329624176025 0.7438045740127563\n",
            "Epoch: 87 --> 0.743932843208313 0.743932843208313 0.7438040375709534\n",
            "Epoch: 88 --> 0.7439327836036682 0.7439327836036682 0.7438029050827026\n",
            "Epoch: 89 --> 0.7439327239990234 0.7439327239990234 0.7438026666641235\n",
            "Epoch: 90 --> 0.7439326643943787 0.7439326643943787 0.7438018918037415\n",
            "Epoch: 91 --> 0.7439326643943787 0.7439326643943787 0.7438013553619385\n",
            "Epoch: 92 --> 0.7439325451850891 0.7439325451850891 0.7438009977340698\n",
            "Epoch: 93 --> 0.7439325451850891 0.7439325451850891 0.7437999248504639\n",
            "Epoch: 94 --> 0.7439325451850891 0.7439325451850891 0.7437998056411743\n",
            "Epoch: 95 --> 0.7439324855804443 0.7439324855804443 0.7437994480133057\n",
            "Epoch: 96 --> 0.7439324259757996 0.7439324259757996 0.743799090385437\n",
            "Epoch: 97 --> 0.7439323663711548 0.7439323663711548 0.7437987327575684\n",
            "Epoch: 98 --> 0.74393230676651 0.74393230676651 0.743799090385437\n",
            "Epoch: 99 --> 0.7439322471618652 0.7439322471618652 0.743798553943634\n",
            "torch.Size([4, 128])\n",
            "Epoch: 100 --> 0.7439321875572205 0.7439321875572205 0.7437986731529236\n",
            "Finished Training\n",
            "Final Accuracy:  1.0\n",
            "1\n",
            "Epoch: 1 --> 0.7940981388092041 0.7940981388092041 0.7844445705413818\n",
            "Epoch: 2 --> 0.7955877184867859 0.7955877184867859 0.7871003150939941\n",
            "Epoch: 3 --> 0.7878451347351074 0.7878451347351074 0.7704220414161682\n",
            "Epoch: 4 --> 0.7757990956306458 0.7757990956306458 0.7667717337608337\n",
            "Epoch: 5 --> 0.7664993405342102 0.7664993405342102 0.7580534815788269\n",
            "Epoch: 6 --> 0.761522114276886 0.761522114276886 0.7537801265716553\n",
            "Epoch: 7 --> 0.7589773535728455 0.7589773535728455 0.75276780128479\n",
            "Epoch: 8 --> 0.7574508786201477 0.7574508786201477 0.7530184984207153\n",
            "Epoch: 9 --> 0.7562832832336426 0.7562832832336426 0.7572451233863831\n",
            "Epoch: 10 --> 0.7550733089447021 0.7550733089447021 0.7552224397659302\n",
            "Epoch: 11 --> 0.7534378170967102 0.7534378170967102 0.7523578405380249\n",
            "Epoch: 12 --> 0.7518309950828552 0.7518309950828552 0.7510014176368713\n",
            "Epoch: 13 --> 0.7505522966384888 0.7505522966384888 0.75023353099823\n",
            "Epoch: 14 --> 0.7495623826980591 0.7495623826980591 0.7494289875030518\n",
            "Epoch: 15 --> 0.7487994432449341 0.7487994432449341 0.7487186193466187\n",
            "Epoch: 16 --> 0.74822998046875 0.74822998046875 0.7479525804519653\n",
            "Epoch: 17 --> 0.7477841973304749 0.7477841973304749 0.7476276159286499\n",
            "Epoch: 18 --> 0.7474111318588257 0.7474111318588257 0.7478487491607666\n",
            "Epoch: 19 --> 0.7470716238021851 0.7470716238021851 0.7480255961418152\n",
            "Epoch: 20 --> 0.7467577457427979 0.7467577457427979 0.7474387884140015\n",
            "Epoch: 21 --> 0.7464669346809387 0.7464669346809387 0.7463911771774292\n",
            "Epoch: 22 --> 0.7462125420570374 0.7462125420570374 0.7457184195518494\n",
            "Epoch: 23 --> 0.7460030317306519 0.7460030317306519 0.7453531622886658\n",
            "Epoch: 24 --> 0.7458323836326599 0.7458323836326599 0.7451046109199524\n",
            "Epoch: 25 --> 0.7456944584846497 0.7456944584846497 0.7449265122413635\n",
            "Epoch: 26 --> 0.7455830574035645 0.7455830574035645 0.7448060512542725\n",
            "Epoch: 27 --> 0.7454922199249268 0.7454922199249268 0.7447133660316467\n",
            "Epoch: 28 --> 0.7454195022583008 0.7454195022583008 0.7446175813674927\n",
            "Epoch: 29 --> 0.745362401008606 0.745362401008606 0.7445533275604248\n",
            "Epoch: 30 --> 0.7453175783157349 0.7453175783157349 0.7445205450057983\n",
            "Epoch: 31 --> 0.7452813386917114 0.7452813386917114 0.7445017695426941\n",
            "Epoch: 32 --> 0.7452516555786133 0.7452516555786133 0.7445110082626343\n",
            "Epoch: 33 --> 0.7452279925346375 0.7452279925346375 0.7445251941680908\n",
            "Epoch: 34 --> 0.7452086210250854 0.7452086210250854 0.7446192502975464\n",
            "Epoch: 35 --> 0.7451918125152588 0.7451918125152588 0.7449740767478943\n",
            "Epoch: 36 --> 0.7451735734939575 0.7451735734939575 0.7472366094589233\n",
            "Epoch: 37 --> 0.7451035976409912 0.7451035976409912 0.745311439037323\n",
            "Epoch: 38 --> 0.7449630498886108 0.7449630498886108 0.7441468238830566\n",
            "Epoch: 39 --> 0.7447958588600159 0.7447958588600159 0.7440685033798218\n",
            "Epoch: 40 --> 0.7446394562721252 0.7446394562721252 0.7440597414970398\n",
            "Epoch: 41 --> 0.7445083856582642 0.7445083856582642 0.7440630197525024\n",
            "Epoch: 42 --> 0.7444033622741699 0.7444033622741699 0.7440720200538635\n",
            "Epoch: 43 --> 0.7443234920501709 0.7443234920501709 0.7440826892852783\n",
            "Epoch: 44 --> 0.744261622428894 0.744261622428894 0.7440992593765259\n",
            "Epoch: 45 --> 0.7442134618759155 0.7442134618759155 0.744113028049469\n",
            "Epoch: 46 --> 0.7441753149032593 0.7441753149032593 0.7441262602806091\n",
            "Epoch: 47 --> 0.7441446781158447 0.7441446781158447 0.7441376447677612\n",
            "Epoch: 48 --> 0.7441197633743286 0.7441197633743286 0.7441468238830566\n",
            "Epoch: 49 --> 0.7440991401672363 0.7440991401672363 0.7441547513008118\n",
            "Epoch: 50 --> 0.744081974029541 0.744081974029541 0.7441555261611938\n",
            "Epoch: 51 --> 0.7440675497055054 0.7440675497055054 0.7441554665565491\n",
            "Epoch: 52 --> 0.744055449962616 0.744055449962616 0.7441509366035461\n",
            "Epoch: 53 --> 0.744045078754425 0.744045078754425 0.7441418170928955\n",
            "Epoch: 54 --> 0.7440364360809326 0.7440364360809326 0.7441281676292419\n",
            "Epoch: 55 --> 0.7440291047096252 0.7440291047096252 0.7441126108169556\n",
            "Epoch: 56 --> 0.7440229654312134 0.7440229654312134 0.7440934777259827\n",
            "Epoch: 57 --> 0.7440177202224731 0.7440177202224731 0.7440744638442993\n",
            "Epoch: 58 --> 0.7440134286880493 0.7440134286880493 0.7440550923347473\n",
            "Epoch: 59 --> 0.744009792804718 0.744009792804718 0.7440312504768372\n",
            "Epoch: 60 --> 0.744006872177124 0.744006872177124 0.744011640548706\n",
            "Epoch: 61 --> 0.7440047264099121 0.7440047264099121 0.7439896464347839\n",
            "Epoch: 62 --> 0.7440029382705688 0.7440029382705688 0.7439709305763245\n",
            "Epoch: 63 --> 0.7440017461776733 0.7440017461776733 0.7439532279968262\n",
            "Epoch: 64 --> 0.7440009117126465 0.7440009117126465 0.7439373731613159\n",
            "Epoch: 65 --> 0.7440003156661987 0.7440003156661987 0.7439237236976624\n",
            "Epoch: 66 --> 0.7440003156661987 0.7440003156661987 0.7439087629318237\n",
            "Epoch: 67 --> 0.7440004348754883 0.7440004348754883 0.743897557258606\n",
            "Epoch: 68 --> 0.7440007925033569 0.7440007925033569 0.7438862919807434\n",
            "Epoch: 69 --> 0.7440013885498047 0.7440013885498047 0.7438774108886719\n",
            "Epoch: 70 --> 0.7440021634101868 0.7440021634101868 0.7438686490058899\n",
            "Epoch: 71 --> 0.7440029978752136 0.7440029978752136 0.7438609600067139\n",
            "Epoch: 72 --> 0.7440040111541748 0.7440040111541748 0.743853747844696\n",
            "Epoch: 73 --> 0.7440050840377808 0.7440050840377808 0.7438475489616394\n",
            "Epoch: 74 --> 0.7440062761306763 0.7440062761306763 0.7438418865203857\n",
            "Epoch: 75 --> 0.7440075278282166 0.7440075278282166 0.7438367009162903\n",
            "Epoch: 76 --> 0.7440089583396912 0.7440089583396912 0.7438324093818665\n",
            "Epoch: 77 --> 0.7440102100372314 0.7440102100372314 0.7438277006149292\n",
            "Epoch: 78 --> 0.744011640548706 0.744011640548706 0.7438235878944397\n",
            "Epoch: 79 --> 0.7440130114555359 0.7440130114555359 0.7438194751739502\n",
            "Epoch: 80 --> 0.7440145611763 0.7440145611763 0.7438157200813293\n",
            "Epoch: 81 --> 0.7440160512924194 0.7440160512924194 0.7438122630119324\n",
            "Epoch: 82 --> 0.744017481803894 0.744017481803894 0.743809163570404\n",
            "Epoch: 83 --> 0.7440190315246582 0.7440190315246582 0.7438063621520996\n",
            "Epoch: 84 --> 0.7440204620361328 0.7440204620361328 0.7438034415245056\n",
            "Epoch: 85 --> 0.7440219521522522 0.7440219521522522 0.7438010573387146\n",
            "Epoch: 86 --> 0.7440234422683716 0.7440234422683716 0.7437986135482788\n",
            "Epoch: 87 --> 0.744024932384491 0.744024932384491 0.7437962293624878\n",
            "Epoch: 88 --> 0.7440264225006104 0.7440264225006104 0.7437940835952759\n",
            "Epoch: 89 --> 0.744027853012085 0.744027853012085 0.7437918186187744\n",
            "Epoch: 90 --> 0.7440292835235596 0.7440292835235596 0.7437899708747864\n",
            "Epoch: 91 --> 0.7440306544303894 0.7440306544303894 0.7437883615493774\n",
            "Epoch: 92 --> 0.744032084941864 0.744032084941864 0.7437865734100342\n",
            "Epoch: 93 --> 0.7440334558486938 0.7440334558486938 0.7437849044799805\n",
            "Epoch: 94 --> 0.7440347671508789 0.7440347671508789 0.7437832951545715\n",
            "Epoch: 95 --> 0.7440362572669983 0.7440362572669983 0.7437818050384521\n",
            "Epoch: 96 --> 0.7440375685691833 0.7440375685691833 0.743780255317688\n",
            "Epoch: 97 --> 0.7440389394760132 0.7440389394760132 0.7437788844108582\n",
            "Epoch: 98 --> 0.7440402507781982 0.7440402507781982 0.7437775135040283\n",
            "Epoch: 99 --> 0.7440415620803833 0.7440415620803833 0.7437760233879089\n",
            "torch.Size([4, 128])\n",
            "Epoch: 100 --> 0.7440428137779236 0.7440428137779236 0.7437748908996582\n",
            "Finished Training\n",
            "Final Accuracy:  1.0\n",
            "2\n",
            "Epoch: 1 --> 0.7691490650177002 0.7691490650177002 0.7914161086082458\n",
            "Epoch: 2 --> 0.7746744155883789 0.7746744155883789 0.7648464441299438\n",
            "Epoch: 3 --> 0.7805323600769043 0.7805323600769043 0.7672942280769348\n",
            "Epoch: 4 --> 0.764051079750061 0.764051079750061 0.7577756643295288\n",
            "Epoch: 5 --> 0.7513539791107178 0.7513539791107178 0.7540531158447266\n",
            "Epoch: 6 --> 0.7479604482650757 0.7479604482650757 0.7551275491714478\n",
            "Epoch: 7 --> 0.7469599843025208 0.7469599843025208 0.7594177722930908\n",
            "Epoch: 8 --> 0.74662184715271 0.74662184715271 0.7611548900604248\n",
            "Epoch: 9 --> 0.7464402914047241 0.7464402914047241 0.7594735622406006\n",
            "Epoch: 10 --> 0.7462789416313171 0.7462789416313171 0.7569829821586609\n",
            "Epoch: 11 --> 0.7460929155349731 0.7460929155349731 0.7553189992904663\n",
            "Epoch: 12 --> 0.7458432912826538 0.7458432912826538 0.7540817856788635\n",
            "Epoch: 13 --> 0.7455546855926514 0.7455546855926514 0.7523079514503479\n",
            "Epoch: 14 --> 0.7452988028526306 0.7452988028526306 0.7513443827629089\n",
            "Epoch: 15 --> 0.7450932264328003 0.7450932264328003 0.7517433762550354\n",
            "Epoch: 16 --> 0.7449202537536621 0.7449202537536621 0.7516263723373413\n",
            "Epoch: 17 --> 0.7447662353515625 0.7447662353515625 0.7503265738487244\n",
            "Epoch: 18 --> 0.7446321845054626 0.7446321845054626 0.7485483884811401\n",
            "Epoch: 19 --> 0.744523823261261 0.744523823261261 0.747488260269165\n",
            "Epoch: 20 --> 0.744438886642456 0.744438886642456 0.7472665309906006\n",
            "Epoch: 21 --> 0.7443712949752808 0.7443712949752808 0.7473535537719727\n",
            "Epoch: 22 --> 0.7443137764930725 0.7443137764930725 0.7473567128181458\n",
            "Epoch: 23 --> 0.7442631721496582 0.7442631721496582 0.7472296357154846\n",
            "Epoch: 24 --> 0.744217038154602 0.744217038154602 0.7469627857208252\n",
            "Epoch: 25 --> 0.744175374507904 0.744175374507904 0.7465909123420715\n",
            "Epoch: 26 --> 0.7441383004188538 0.7441383004188538 0.7462084889411926\n",
            "Epoch: 27 --> 0.7441059350967407 0.7441059350967407 0.7458350658416748\n",
            "Epoch: 28 --> 0.7440776824951172 0.7440776824951172 0.7455588579177856\n",
            "Epoch: 29 --> 0.7440530061721802 0.7440530061721802 0.7453503012657166\n",
            "Epoch: 30 --> 0.7440316081047058 0.7440316081047058 0.7451956272125244\n",
            "Epoch: 31 --> 0.7440131306648254 0.7440131306648254 0.7451127171516418\n",
            "Epoch: 32 --> 0.7439968585968018 0.7439968585968018 0.7450722455978394\n",
            "Epoch: 33 --> 0.7439824938774109 0.7439824938774109 0.745050311088562\n",
            "Epoch: 34 --> 0.7439695000648499 0.7439695000648499 0.7450186610221863\n",
            "Epoch: 35 --> 0.7439578175544739 0.7439578175544739 0.7449831962585449\n",
            "Epoch: 36 --> 0.7439470291137695 0.7439470291137695 0.7449330687522888\n",
            "Epoch: 37 --> 0.7439369559288025 0.7439369559288025 0.7448809146881104\n",
            "Epoch: 38 --> 0.7439276576042175 0.7439276576042175 0.7448219060897827\n",
            "Epoch: 39 --> 0.7439191341400146 0.7439191341400146 0.7447625398635864\n",
            "Epoch: 40 --> 0.7439112663269043 0.7439112663269043 0.7447090148925781\n",
            "Epoch: 41 --> 0.7439039945602417 0.7439039945602417 0.744661808013916\n",
            "Epoch: 42 --> 0.7438971996307373 0.7438971996307373 0.7446211576461792\n",
            "Epoch: 43 --> 0.7438909411430359 0.7438909411430359 0.7445887327194214\n",
            "Epoch: 44 --> 0.7438852190971375 0.7438852190971375 0.7445601224899292\n",
            "Epoch: 45 --> 0.7438798546791077 0.7438798546791077 0.7445358037948608\n",
            "Epoch: 46 --> 0.7438749074935913 0.7438749074935913 0.7445154190063477\n",
            "Epoch: 47 --> 0.7438701391220093 0.7438701391220093 0.7444980144500732\n",
            "Epoch: 48 --> 0.7438657283782959 0.7438657283782959 0.7444806098937988\n",
            "Epoch: 49 --> 0.7438615560531616 0.7438615560531616 0.7444655299186707\n",
            "Epoch: 50 --> 0.7438576221466064 0.7438576221466064 0.7444507479667664\n",
            "Epoch: 51 --> 0.7438538670539856 0.7438538670539856 0.7444349527359009\n",
            "Epoch: 52 --> 0.7438503503799438 0.7438503503799438 0.7444201707839966\n",
            "Epoch: 53 --> 0.7438470125198364 0.7438470125198364 0.7444055676460266\n",
            "Epoch: 54 --> 0.7438437342643738 0.7438437342643738 0.7443907260894775\n",
            "Epoch: 55 --> 0.7438405752182007 0.7438405752182007 0.7443745136260986\n",
            "Epoch: 56 --> 0.7438375949859619 0.7438375949859619 0.7443594336509705\n",
            "Epoch: 57 --> 0.7438348531723022 0.7438348531723022 0.7443447113037109\n",
            "Epoch: 58 --> 0.7438321709632874 0.7438321709632874 0.7443298101425171\n",
            "Epoch: 59 --> 0.7438295483589172 0.7438295483589172 0.744316041469574\n",
            "Epoch: 60 --> 0.7438271045684814 0.7438271045684814 0.7443033456802368\n",
            "Epoch: 61 --> 0.7438247203826904 0.7438247203826904 0.7442921996116638\n",
            "Epoch: 62 --> 0.7438225746154785 0.7438225746154785 0.7442810535430908\n",
            "Epoch: 63 --> 0.7438203692436218 0.7438203692436218 0.7442710399627686\n",
            "Epoch: 64 --> 0.7438181638717651 0.7438181638717651 0.7442612648010254\n",
            "Epoch: 65 --> 0.7438162565231323 0.7438162565231323 0.7442525625228882\n",
            "Epoch: 66 --> 0.7438142895698547 0.7438142895698547 0.7442449331283569\n",
            "Epoch: 67 --> 0.7438124418258667 0.7438124418258667 0.7442371249198914\n",
            "Epoch: 68 --> 0.7438106536865234 0.7438106536865234 0.7442294359207153\n",
            "Epoch: 69 --> 0.7438089847564697 0.7438089847564697 0.7442226409912109\n",
            "Epoch: 70 --> 0.7438071966171265 0.7438071966171265 0.7442152500152588\n",
            "Epoch: 71 --> 0.7438055276870728 0.7438055276870728 0.7442086935043335\n",
            "Epoch: 72 --> 0.7438040971755981 0.7438040971755981 0.7442022562026978\n",
            "Epoch: 73 --> 0.7438024878501892 0.7438024878501892 0.7441959381103516\n",
            "Epoch: 74 --> 0.7438009977340698 0.7438009977340698 0.7441903352737427\n",
            "Epoch: 75 --> 0.7437995076179504 0.7437995076179504 0.7441843748092651\n",
            "Epoch: 76 --> 0.7437981367111206 0.7437981367111206 0.7441784143447876\n",
            "Epoch: 77 --> 0.7437967658042908 0.7437967658042908 0.7441731691360474\n",
            "Epoch: 78 --> 0.7437955141067505 0.7437955141067505 0.7441675066947937\n",
            "Epoch: 79 --> 0.7437942028045654 0.7437942028045654 0.7441624402999878\n",
            "Epoch: 80 --> 0.7437928318977356 0.7437928318977356 0.7441571354866028\n",
            "Epoch: 81 --> 0.7437915802001953 0.7437915802001953 0.7441518306732178\n",
            "Epoch: 82 --> 0.7437905073165894 0.7437905073165894 0.7441466450691223\n",
            "Epoch: 83 --> 0.7437891960144043 0.7437891960144043 0.7441416382789612\n",
            "Epoch: 84 --> 0.7437881231307983 0.7437881231307983 0.7441369295120239\n",
            "Epoch: 85 --> 0.7437869906425476 0.7437869906425476 0.744132399559021\n",
            "Epoch: 86 --> 0.7437859177589417 0.7437859177589417 0.7441279888153076\n",
            "Epoch: 87 --> 0.7437848448753357 0.7437848448753357 0.7441235184669495\n",
            "Epoch: 88 --> 0.743783712387085 0.743783712387085 0.7441197037696838\n",
            "Epoch: 89 --> 0.7437826991081238 0.7437826991081238 0.7441154718399048\n",
            "Epoch: 90 --> 0.7437816858291626 0.7437816858291626 0.7441115379333496\n",
            "Epoch: 91 --> 0.7437807321548462 0.7437807321548462 0.7441074848175049\n",
            "Epoch: 92 --> 0.7437798976898193 0.7437798976898193 0.7441036105155945\n",
            "Epoch: 93 --> 0.7437788844108582 0.7437788844108582 0.744100034236908\n",
            "Epoch: 94 --> 0.7437779903411865 0.7437779903411865 0.7440963387489319\n",
            "Epoch: 95 --> 0.7437770366668701 0.7437770366668701 0.744092583656311\n",
            "Epoch: 96 --> 0.7437762022018433 0.7437762022018433 0.7440893650054932\n",
            "Epoch: 97 --> 0.7437752485275269 0.7437752485275269 0.7440856695175171\n",
            "Epoch: 98 --> 0.7437743544578552 0.7437743544578552 0.7440824508666992\n",
            "Epoch: 99 --> 0.7437736392021179 0.7437736392021179 0.7440791726112366\n",
            "torch.Size([4, 128])\n",
            "Epoch: 100 --> 0.7437727451324463 0.7437727451324463 0.74407559633255\n",
            "Finished Training\n",
            "Final Accuracy:  1.0\n",
            "3\n",
            "Epoch: 1 --> 0.803097128868103 0.803097128868103 0.811883807182312\n",
            "Epoch: 2 --> 0.8027717471122742 0.8027717471122742 0.8160897493362427\n",
            "Epoch: 3 --> 0.7878018021583557 0.7878018021583557 0.7691569924354553\n",
            "Epoch: 4 --> 0.7697432637214661 0.7697432637214661 0.7595703601837158\n",
            "Epoch: 5 --> 0.7606251239776611 0.7606251239776611 0.7612057328224182\n",
            "Epoch: 6 --> 0.7560785412788391 0.7560785412788391 0.7615976333618164\n",
            "Epoch: 7 --> 0.7535747289657593 0.7535747289657593 0.7675599455833435\n",
            "Epoch: 8 --> 0.7518150806427002 0.7518150806427002 0.7574337720870972\n",
            "Epoch: 9 --> 0.7501533031463623 0.7501533031463623 0.751826286315918\n",
            "Epoch: 10 --> 0.7489051818847656 0.7489051818847656 0.7505664825439453\n",
            "Epoch: 11 --> 0.7480697631835938 0.7480697631835938 0.7507628798484802\n",
            "Epoch: 12 --> 0.7474695444107056 0.7474695444107056 0.7509031295776367\n",
            "Epoch: 13 --> 0.7470000386238098 0.7470000386238098 0.7503200173377991\n",
            "Epoch: 14 --> 0.7466241717338562 0.7466241717338562 0.7489678859710693\n",
            "Epoch: 15 --> 0.7463282346725464 0.7463282346725464 0.7476350665092468\n",
            "Epoch: 16 --> 0.7460997104644775 0.7460997104644775 0.7467980980873108\n",
            "Epoch: 17 --> 0.7459239363670349 0.7459239363670349 0.7469424605369568\n",
            "Epoch: 18 --> 0.7457572221755981 0.7457572221755981 0.7507341504096985\n",
            "Epoch: 19 --> 0.7455846667289734 0.7455846667289734 0.7471548318862915\n",
            "Epoch: 20 --> 0.7454065680503845 0.7454065680503845 0.7457760572433472\n",
            "Epoch: 21 --> 0.745254397392273 0.745254397392273 0.7452831268310547\n",
            "Epoch: 22 --> 0.7451295852661133 0.7451295852661133 0.7449964284896851\n",
            "Epoch: 23 --> 0.7450281381607056 0.7450281381607056 0.7448166608810425\n",
            "Epoch: 24 --> 0.7449442148208618 0.7449442148208618 0.7446831464767456\n",
            "Epoch: 25 --> 0.7448748350143433 0.7448748350143433 0.7445796728134155\n",
            "Epoch: 26 --> 0.7448171377182007 0.7448171377182007 0.7444899678230286\n",
            "Epoch: 27 --> 0.7447694540023804 0.7447694540023804 0.7444262504577637\n",
            "Epoch: 28 --> 0.7447287440299988 0.7447287440299988 0.7443658113479614\n",
            "Epoch: 29 --> 0.7446944117546082 0.7446944117546082 0.744314968585968\n",
            "Epoch: 30 --> 0.7446647882461548 0.7446647882461548 0.7442700862884521\n",
            "Epoch: 31 --> 0.744638979434967 0.744638979434967 0.7442306876182556\n",
            "Epoch: 32 --> 0.7446166276931763 0.7446166276931763 0.7441952228546143\n",
            "Epoch: 33 --> 0.7445971965789795 0.7445971965789795 0.7441655993461609\n",
            "Epoch: 34 --> 0.744580090045929 0.744580090045929 0.744138240814209\n",
            "Epoch: 35 --> 0.7445650100708008 0.7445650100708008 0.744112491607666\n",
            "Epoch: 36 --> 0.7445517182350159 0.7445517182350159 0.7440885305404663\n",
            "Epoch: 37 --> 0.7445399761199951 0.7445399761199951 0.7440677881240845\n",
            "Epoch: 38 --> 0.7445297837257385 0.7445297837257385 0.7440506815910339\n",
            "Epoch: 39 --> 0.7445206642150879 0.7445206642150879 0.7440329194068909\n",
            "Epoch: 40 --> 0.7445127367973328 0.7445127367973328 0.7440171241760254\n",
            "Epoch: 41 --> 0.7445056438446045 0.7445056438446045 0.744003176689148\n",
            "Epoch: 42 --> 0.7444994449615479 0.7444994449615479 0.7439895868301392\n",
            "Epoch: 43 --> 0.7444939613342285 0.7444939613342285 0.7439768314361572\n",
            "Epoch: 44 --> 0.744489312171936 0.744489312171936 0.7439664006233215\n",
            "Epoch: 45 --> 0.7444850206375122 0.7444850206375122 0.7439565658569336\n",
            "Epoch: 46 --> 0.7444813847541809 0.7444813847541809 0.7439473271369934\n",
            "Epoch: 47 --> 0.7444780468940735 0.7444780468940735 0.7439383864402771\n",
            "Epoch: 48 --> 0.7444753050804138 0.7444753050804138 0.743929922580719\n",
            "Epoch: 49 --> 0.7444729208946228 0.7444729208946228 0.7439228296279907\n",
            "Epoch: 50 --> 0.7444708347320557 0.7444708347320557 0.7439159750938416\n",
            "Epoch: 51 --> 0.7444690465927124 0.7444690465927124 0.7439095377922058\n",
            "Epoch: 52 --> 0.744467556476593 0.744467556476593 0.7439036965370178\n",
            "Epoch: 53 --> 0.7444663047790527 0.7444663047790527 0.7438982725143433\n",
            "Epoch: 54 --> 0.7444652318954468 0.7444652318954468 0.7438932061195374\n",
            "Epoch: 55 --> 0.7444643974304199 0.7444643974304199 0.7438878417015076\n",
            "Epoch: 56 --> 0.7444637417793274 0.7444637417793274 0.7438833117485046\n",
            "Epoch: 57 --> 0.7444632053375244 0.7444632053375244 0.7438788414001465\n",
            "Epoch: 58 --> 0.7444629073143005 0.7444629073143005 0.7438745498657227\n",
            "Epoch: 59 --> 0.7444626688957214 0.7444626688957214 0.7438703775405884\n",
            "Epoch: 60 --> 0.7444626092910767 0.7444626092910767 0.7438663244247437\n",
            "Epoch: 61 --> 0.7444626688957214 0.7444626688957214 0.7438626289367676\n",
            "Epoch: 62 --> 0.7444628477096558 0.7444628477096558 0.7438591718673706\n",
            "Epoch: 63 --> 0.7444630861282349 0.7444630861282349 0.7438558340072632\n",
            "Epoch: 64 --> 0.7444634437561035 0.7444634437561035 0.7438527345657349\n",
            "Epoch: 65 --> 0.7444638013839722 0.7444638013839722 0.74385005235672\n",
            "Epoch: 66 --> 0.7444643974304199 0.7444643974304199 0.7438472509384155\n",
            "Epoch: 67 --> 0.7444649934768677 0.7444649934768677 0.7438446879386902\n",
            "Epoch: 68 --> 0.7444656491279602 0.7444656491279602 0.7438421249389648\n",
            "Epoch: 69 --> 0.7444663643836975 0.7444663643836975 0.7438393831253052\n",
            "Epoch: 70 --> 0.7444671392440796 0.7444671392440796 0.7438371181488037\n",
            "Epoch: 71 --> 0.7444680333137512 0.7444680333137512 0.7438349723815918\n",
            "Epoch: 72 --> 0.7444689273834229 0.7444689273834229 0.743833065032959\n",
            "Epoch: 73 --> 0.7444698810577393 0.7444698810577393 0.7438310384750366\n",
            "Epoch: 74 --> 0.7444709539413452 0.7444709539413452 0.7438289523124695\n",
            "Epoch: 75 --> 0.7444720268249512 0.7444720268249512 0.743827223777771\n",
            "Epoch: 76 --> 0.7444730401039124 0.7444730401039124 0.7438254356384277\n",
            "Epoch: 77 --> 0.7444742918014526 0.7444742918014526 0.7438238859176636\n",
            "Epoch: 78 --> 0.7444753646850586 0.7444753646850586 0.7438226938247681\n",
            "Epoch: 79 --> 0.7444765567779541 0.7444765567779541 0.7438210248947144\n",
            "Epoch: 80 --> 0.7444779276847839 0.7444779276847839 0.7438195943832397\n",
            "Epoch: 81 --> 0.7444791197776794 0.7444791197776794 0.7438183426856995\n",
            "Epoch: 82 --> 0.744480550289154 0.744480550289154 0.7438170909881592\n",
            "Epoch: 83 --> 0.7444818019866943 0.7444818019866943 0.7438157796859741\n",
            "Epoch: 84 --> 0.7444832921028137 0.7444832921028137 0.74381422996521\n",
            "Epoch: 85 --> 0.7444848418235779 0.7444848418235779 0.7438129186630249\n",
            "Epoch: 86 --> 0.7444863319396973 0.7444863319396973 0.7438119649887085\n",
            "Epoch: 87 --> 0.7444877624511719 0.7444877624511719 0.7438108921051025\n",
            "Epoch: 88 --> 0.7444894909858704 0.7444894909858704 0.7438099384307861\n",
            "Epoch: 89 --> 0.7444910407066345 0.7444910407066345 0.7438092231750488\n",
            "Epoch: 90 --> 0.7444927096366882 0.7444927096366882 0.743808388710022\n",
            "Epoch: 91 --> 0.7444944381713867 0.7444944381713867 0.7438074350357056\n",
            "Epoch: 92 --> 0.7444961667060852 0.7444961667060852 0.7438067197799683\n",
            "Epoch: 93 --> 0.7444979548454285 0.7444979548454285 0.7438062429428101\n",
            "Epoch: 94 --> 0.7444998025894165 0.7444998025894165 0.7438052892684937\n",
            "Epoch: 95 --> 0.7445017099380493 0.7445017099380493 0.7438052892684937\n",
            "Epoch: 96 --> 0.7445036172866821 0.7445036172866821 0.7438052296638489\n",
            "Epoch: 97 --> 0.7445055842399597 0.7445055842399597 0.7438058257102966\n",
            "Epoch: 98 --> 0.7445075511932373 0.7445075511932373 0.7438065409660339\n",
            "Epoch: 99 --> 0.7445095181465149 0.7445095181465149 0.7438067197799683\n",
            "torch.Size([4, 128])\n",
            "Epoch: 100 --> 0.744511604309082 0.744511604309082 0.7438086271286011\n",
            "Finished Training\n",
            "Final Accuracy:  1.0\n",
            "4\n"
          ]
        }
      ]
    }
  ]
}